\chapter{e1000 paravirtualization}
\label{cha:paravirt}
In chapter \ref{cha:e1000-opt} we have proposed two simple patches that boost the e1000 performances.
The moderation patch involves only modification to the hypervisor, while the batching patch involves only modification to the
guest device driver. These patches can be applied independently on each other, even if the batching generally works better if
it comes together with moderation.

Neverthless, in both cases the we have respected the original e1000 specification, so that the guest can use the e1000 adapter with its
original (or patched) driver and be unaware that it is actually in a Virtual Machine environment, and that the e1000 adapter is
emulated. This unawareness is the essence of the \emph{full virtualization} concept: The guest doesn't know to be emulated, so
that we can run an unmodified OS on top of it and everything works fine.

\vspace{0.5cm}

Another approach that sometimes is used is \emph{paravirtualization}, a general concept that describes situations in which the 
guest is aware of being in a Virtual Machine environment, and cooperates with the hypervisor in order to make the virtualization simpler
and/or to get better performances.


\section{Device paravirtualization}
In this thesis we are interested in device paravirtualization, and in particular in network device paravirtualization.
With this kind of paravirtualization, only the \emph{paravirtualized} device driver is aware of the virtualization, while the rest of the
system is not.
We can obtain a paravirtualized driver either by modifying an existing real driver, or by creating a new \emph{fake} driver. In the
latter case the driver correspond to a new virtual (fake) device that does not really exist, but is just a stub used to communicate with 
the outside world exporting to the guest OS the same interface exported by a real driver, so that the guest OS
can make use of it without being aware that the device is a fake one.
In both cases the paravirtualization requires hypervisor support.

In our case we could modify the e1000 Linux driver and the e1000 QEMU frontend, or we could create a new Linux network driver and
a new QEMU frontend.

\vspace{0.5cm}

How can paravirtualization improve performances? Device hardware specifications are generally very complex, and often
inlcude a lot of physical details, offloading capabilities and other hardware related features. When emulating the device, most of these 
details and features are just useless, or is not worth/possible implementing it. Moreover, the devices communicate with the OS mainly 
through register accesses because register accesses are not expensive in hardware, but they do are expensive within emulators, since
they cause VMExits. In other words, emulating real device is complicated and inefficient.

\vspace{0.5cm}

The idea behind paravirtualization is that most of hardware-related details are source of useless overhead, and this overhead can be 
easily avoided if the driver knows that it is talking to a virtual device and not to a real hardware. As an example, the e1000 driver has
to do at least 5 VMExit while executing the interrupt routine, but if we knew that the e1000 device is virtual, some (or all) of these
would become useless, or at least could be replaced with something cheaper.

The purpose of a paravirtualized devices is therefore to estabilish an efficient communication between the device driver and the emulator
(e.g. the QEMU frontend), while the guest OS think of the driver as being a driver of a real device.
In order to make the communication efficient, we have to minimize VMExits, and so register accesses. The communication should be done, when
possible, through shared memory. The TX ring is an example of shared memory used for communication: The driver writes to the TX ring and
the frontend reads from it (and then also writes back). The RX ring is another example.

However, also a paravirtualized driver needs to access register, since a register access is the only way the driver can notify the
emulator to start some processing or, in general, to have side effects. The difference with a real driver is that a paravirtualized one
does a register access only when is really essential, e.g. for notifications. For everything else, the communication is done through
shared memory. A real device driver doesn't worry so much about accessing registers.



\section{The Virtio standard}
Virtio is a virtualization standard that aims at high I/O performances through device paravirtualization. This is done creating completely
new device drivers, which are able to communicate efficiently with a Virtio-capable hypervisor. Its approach is
similar to the Xen I/O paravirtualization and the VMware \emph{Guest Tools}.
Virtio is an effort to estabilish a standard interface between drivers and hypervisors for paravirtualized I/O, in order to increase the 
code reuse across platform. In this way we avoid to have an independent I/O paravirtualization solution for each hypervisor.
The Virtio standard defines different I/O (fake) devices, including a network adapter, a SCSI disk and a serial interface, and is currently
supported by QEMU and Virtualbox. Linux and Windows drivers are available for Virtio devices.

\begin{figure}[bt]
\centering
\includegraphics[scale = 0.45]{virtio.pdf}
\caption{A guest and its hypervisor communicate through the Virtio interface.}
\label{fig:virtio}
\end{figure}

As we can see from figure \ref{fig:virtio}, the Virtio interface is implemented through a virtio frontend in the guest OS, and a virtio
backend in the hypervisor. All the virtio device drivers can share the virtio frontend code. The task of a virtio driver is therefore to
convert the OS representation of the data to exchange (e.g. a network packet) into the standard virtio data format, or the other way
around. All the virtio device emulators (e.g. the QEMU frontend for each virtio device) can share the virtio backend code. The task
of a virtio device emulator is therefore to convert the virtio representation of the data to exchange into the hypervisor specific
representation (e.g. a buffer containg an ethernet frame), or the other way around. In this way, the guest OS and the hypervisor can
communicate through the virtio infrastructure using an efficient and general mechanism.

\vspace{0.5cm}

The organization illustrated in figure \ref{fig:virtio} is actually similar to the one used for real device emulation (e.g. e1000 standard).
However the Virtio interface is explicitely designed for efficient communication between driver and hypervisor, while this is not true for
real device hardware/software interfaces.


\subsection{Virtual queues}
\label{sec:virtqueue}
Central to the Virtio interface is the \emph{virtual queue} abstraction, a queue that connects a virtio frontend to a virtio backend.
A virtual queue is simply a queue into which buffers are posted by the guest driver for consumption by the hypervisor: In this way
the two peers can exchange data.
A virtual queue can be used to exchange data in both directions: Therefore the posted buffers can be used both for output and for input 
operations. Drivers can use zero, one or more queues, depending on their needs. For example, the virtio network device uses two virtual
queues (one for receive and one for transmit), while the virtio block device uses only one.
The buffers use to exchange data are represented in Virtio using scatter-gather lists\footnote{Each
element in the list represents the guest physical address and a length of a physical contiguous chunk of memory}. With a single operation
a virtio frontend can send a scatter-gather list to a virtio backend. A single scatter-gather list can specify both input and output 
requests. For example, the guest may send to the hypervisor a SG-list containing three buffers: An output buffer that specifies a 
command and two input buffer that will be filled by the hypervisor with the response\footnote{This example could be valid for a virtio
disk.}.

\vspace{0.5cm}

In more detail, when the guest wants to make requests to the hypervisor through a virtual queue, it invokes the \texttt{add\_buf} method
on the virtual queue object, passing a SG-list and a non-NULL token which is returned when the SG has been consumed. As we have seen 
previously, a single SG-list can be used to pass many output buffers (e.g. network packets to send) and many input buffers (e.g. memory 
buffers where the hypervisor can store received network packets). The method returns the amount of space left in the queue, so that 
the guest can stop adding new buffers when the queue is full.
The \texttt{add\_buf} method doesn't notify the hypervisor about the new requests, but only inserts the new buffer in the ring associated
with the virtual queue object. When a virtio driver wants to notify the hypervisor, it has to call the \texttt{kick} method. Of course
the driver should kick the hypervisor only when necessary, and try to add as many buffers as possible before \emph{kicking} the hypervisor.

When the hypervisor is notified, it extracts (pops) an SG list from the virtual queue and process the request, maybe asynchronously.
When the processing is done, the hypervisor returns the used SG list to the virtual queue.
The guest can poll for the request completion through the \texttt{get\_buf} function. This function is not blocking and returns NULL if 
there are no returned used SG lists, or returns the token of a SG list that has been consumed. Only when the \texttt{get\_buf} is called
the queue space used by a previous \texttt{add\_buf} is freed.

The whole process is depicted in figure \ref{fig:virtqueue}.

\begin{figure}[bt]
\centering
\includegraphics[scale = 0.48]{virtqueue.pdf}
\caption{Virtual queue operations.}
\label{fig:virtqueue}
\end{figure}

\vspace{0.5cm}

In order to avoid busy waiting, the driver can provide a callback function to a virtual queue. This callback will be invoked when the
hypervisor notifies that new used buffers have been returned. Since hypervisor notifications are generally expensive (in QEMU-KVM they are 
implemented as interrupts to the guest, so they are extremely expensive), the driver should implement strategies aimed at mitigating the
notification rate (e.g. with NAPI). In order to do that, the driver can enable or disable callbacks (e.g. enable/disable interrupts) 
invoking the \texttt{enable\_cb} or \texttt{disable\_cb} methods on the virtual queue. The \texttt{disable\_cb} is actually only a hint,
there is no guarantee that the callback will not still be called right after, since this would require expensive synchronization: It is
just an optimization to reduce unnecessary notifications.
The callback function can call the \texttt{get\_buf} so that it can process the used buffers.

\vspace{0.5cm}

In a similar way, the hypervisor can enable/disable guest notifications (kicks) \footnote{e.g. In our working environment this are
implemented as guest MMIO accesses.}, since also this notifications are very expensive. When guest notifications are disabled,
the \texttt{kick} method has no effect.


\subsection{The virtio ring transport mechanism}
In the current implementation, the virtual queues are implemented as \emph{rings}, similarly to what happens with network adapters (see
section \ref{sec:e1000-interface}). The Virtio interface hides the rings, so that we could use a different implementation as
long as both virtio frontend and virtio backend use the same transport mechanism.
Since a virtio ring must be a very general and flexible mechanism for transporting buffers, it is also quite complex, or at least more
complex than needed for and efficient paravirtualized network device.

\vspace{0.5cm}

A virtio ring consists of three parts: the descriptor array where the guest chains together length/address pairs (taken from a guest-provided
SG list), the \emph{available} ring where the guest indicates what descriptors chains are ready for use, and the \emph{used} ring where the
host indicates which descriptors chains it has used. Each descriptor contains the guest-physical address of the buffer, its length, an 
optional \texttt{next} index for buffer chaining, and two flags: one to indicate whether the next field is valid and one controlling 
whether the buffer is read-only or write-only. This allows a chained buffer to contain both readable and writable parts (this is 
useful for implementing a block device).
The available ring consists of a free-running index (the \emph{avail} index)\footnote{The ring index is a 16 bit unsigned integer, 
which is always incremented and is intended to overflow.}, an interrupt suppression flag, and an array of indices into the descriptor array 
where each index references the head of a descriptor chains. As we can see, the available ring is separated from the descriptor array, 
adding another level of indirection: The avail index references an entry of the avail ring and this entry references an entry of the
descriptor array (an head of a descriptor chain).
This separation of the descriptor array from the available ring is due to the asynchronous nature of the virtqueue. In fact the hypervisor
extracts the chains in the same order in which they have been inserted, but may process them in a different order and/or asynchronously.
In this case some chains could require more time than others, and so the available ring could circle many times with fast-serviced
chains while slow descriptors might still await completion (this is useful for block device implementation).
The interrupt suppression flag is set when the virtio driver invokes the \texttt{disable\_cb} and is suppressed when the
\texttt{enable\_cb} is invoked. In this way the guest can implement a mitigation strategy for host notification (section 
\ref{sec:virtqueue}).
The used ring has the same structure of the the available ring, but is written by the host as descriptor chains are consumed. Moreover,
the each entry of used ring contains not only an index in the descriptor array, but also a \texttt{len} field which is used for input
buffers: When an input buffer has been used, the hypervisor writes in this field the size of the read operation which wrote to the buffer.

The data structures are depicted in figure \ref{fig:vring}.

\begin{figure}[bt]
\centering
\includegraphics[scale = 0.48]{vring.pdf}
\caption{Virtio ring data structures. On the top the descriptor array is depicted as a table. On the bottom the \emph{avail} ring and the 
	 \emph{used} ring.}
\label{fig:vring}
\end{figure}

\vspace{0.5cm}

Putting all together, let's describe the journey of a virtio buffer in a virtual queue. A virtio driver invokes \texttt{add\_buf} on the
virtual queue, and the provided SG list is inserted in the descriptor array, using a free entry for each element in the SG list. All this
descriptors are chained together. At this point a new entry is inserted in the avail ring, referencing the head of the chain just now
built, and the avail index is incremented.
Now the virtio driver can kick the hypervisor (if is the case), because there is a new entry in the avail ring.
When the hypervisor sees that there is work to do, it pops the new entry from the available ring, processes it, pushes the used descriptor
chain in the used ring, increments the used index and, if is the case, notifies the guest (e.g. send an interrupt). When the guest wants 
to get an used buffer, it invokes the \texttt{get\_buf} method so that it can clean or complete to process the returned buffer.


%\subsubsection{The VIRTIO\_RING\_F\_EVENT\_IDX feature}


\subsection{Minimizing notifications using Virtio}
\label{sec:virtiomin}
Let's see how we can build an Virtio driver that tries to minimize the notification rate in both directions, assuming we are dealing with
our usual working environment (QEMUKVM as VMM and Linux as guest).
As we've seen so far, in our working environment notifications are very expensive, and so minimizing them leads to big performance
improvements. Since Virtio is explicitely designed to work in a virtualized environment, it's easy to write a driver that minimizes
notifications.
We will make an abstract example where the virtio driver, using a single virtual queue, receives requests from the kernel and passes
these requests (represented as virtio buffers) to the hypervisor. The hypervisor processes these requests in a dedicated thread and returns 
the used buffers to the virtio driver. When the guest is notified by the hypervisor, the virtio driver gets the used buffers and do some
post processing. To be more precise, we would like to minimize the ratio between the average notification rate and the average request rate,
e.g. maximize the percentage of spared notifications.

\vspace{0.5cm}

\begin{figure}[bt]
\centering
\includegraphics[scale = 1.0]{virtiocode.pdf}
\caption{Pseudocode for the virtio driver and the virtio device emulation for our example.}
\label{fig:virtiocode}
\end{figure}

The pseudocode for the virtio driver and the corresponding device emulation into the hypervisor is reported in figure
\ref{fig:virtiocode}. This pseudocode skips many details, but contains all the interesting strategies to minimize notifications.
The request processing in the hypervisor is done in the IOThread, using a QEMUBH (section \ref{sec:qemuel}). The QEMUBH is scheduled
in the \texttt{notification\_callback} function, which is executed by a VCPU thread after a VMExit due to a guest notification.
As we have already seen, the guest can notify the hypervisor using the \texttt{kick} method, which turns into a real notification
(e.g. a VMExit) only if the hypervisor has the notifications enabled.
The postprocessing is done by the NAPI polling function, which runs in a dedicated thread, and is scheduled by the virtqueue callback
(\texttt{virtqueue\_callback} function). The callback is executed in interrupt context when the hypervisor notifies the guest with
an interrupt.

\vspace{0.5cm}

In conclusion, there is a dedicate worker both in the host (the QEMUBH) and in the guest (the NAPI polling function). Each worker runs
in a separate thread, so that there is parallelism. When the guest notifies the host, the host worker starts its processing and stops
only when there is no more work. When the host notifies the guest, the guest worker starts its processing and stops only when there is
no more work. Note that the processing system is symmetric: There are two symmetric \emph{producer/consumer couples} (in short PCCs). In 
the first PCC, the producer is the guest kernel which continuosly invokes \texttt{request()}, and the consumer is the QEMUBH, which 
continuously processes (comsumes) the requests. In the second PCC, the producer is the QEMUBH itself which invokes 
\texttt{virtqueue.push()} and the consumer is the NAPI polling function that does the post processing.

\vspace{0.5cm}

How are the notification minimized? Using the very same ideas introduced by NAPI (see section \ref{sec:napi}): When a notification comes,
if the notification rate is high because the incoming work request is high, we disable notifications and start polling for
requests. While there are requests to process we keep processing, keeping the notifications disabled. When there are no requests left, we
exit the working cycle and reenable notifications, so that we can be notified in the future when new requests will come.
This concept is employed for both the PCCs. To be complete, after the notification have been enabled the consumer 
checks again if there is other work to do. If true the notifications are disabled and the consumer reschedules itself. This is done because
of a race condition that we will show in the following ``Race condition analysis'' subsection.
Using this system, if the consumer is slower than the producer (see the discussion repoted in section \ref{sec:e1000-rx-g2h1vcpu}), the 
performances of the consumer/producer are very good.

\vspace{0.5cm}

Let's analyze how this system works, assuming the producers slower than the consumers. At the beginning notifications are 
enabled in both driver and device emulator.As the kernel invokes \texttt{request} for the first time, the request is added to the 
virtualqueue and the kick notifies the hypervisor. Because of the
notification, \texttt{notification\_callback} is invoked: Further guest notifications are disabled and the QEMUBH worker is
scheduled. The QEMUBH worker starts processing requests and keep doing it as the virtqueue avail ring is not empty (and because of our 
assumptions this is very unlikely to be empty). After the first request has been processed, the host successfully notifies the guest 
(\texttt{virtqueue.notify()}), and therefore \texttt{virtqueue\_callback} is invoked: Further host notifications are disabled and the
NAPI polling function is scheduled. The polling worker start to post-process requests and keep doing it as the used ring is not empty 
(and it is unlikely to be so). As we can see, this system can, in theory, work without notifications, except for the first two ones.
Of course this is not realistic, also because the \texttt{request} function could implement some form of stream control and tell the kernel
to stop sending requests when it sees that the avail ring is full (this will happen because the consumer is fast). If requests stop, also
the two consumers stop, notification are reenabled and the kernel can restart sending request (causing other notifications).
Neverthless, the notification rate is still very low w.r.t. the request rate.


\subsubsection{Race condition analysis}
The consumer working cycle exits when there is no work left. Once exited, the notifications are enabled. The race exists because these
two operations, namely checking for more work and enabling notifications are not atomic. If in beetween these operation the producer
inserts more request in the virtual queue and tries to notify the consumer, the notification of this last request is not done because 
notifications are disabled. If the consumer does not double check for more work after enabling notifications, and the consumer doesn't
add other requests for an hour, tha last request stalls in the queue for an hour. Since the consumer double checks, however, it sees
that new requests have come while it was not polling and reschedules itself (and possibly consume the request immediately). In this
way request cannot stall.

\begin{figure}[bt]
\centering
\includegraphics[scale = 1.0]{race.pdf}
\caption{Race condition between a producer and a consumer.}
\label{fig:race}
\end{figure}


\subsubsection{Other details}
In our example we've skipped many details in order to keep the pseudocode simple. However, it's important to note that both the NAPI
polling function and the QEMUBH, in their working cycle, must limit the amount of work done, otherwise they would monopolize the VCPU
thread (in the NAPI case) or the IOThread (in the QEMUBH case). For these reason the workers must count the requests processed and
exit the working cycle when the count exceeds a given amount. In the NAPI case, this amount is passed to the polling function in the
\texttt{budget} argument, whereas in the QEMUBH case we have to choose a value (e.g. 256 is a good compromise between performances and
responsiveness of the event-loop).



\section{An efficient Virtio network driver}
\label{sec:virtionet}
Using the ideas presented in section \ref{sec:virtiomin} one can build a very efficient virtio network driver and network device emulator 
(\emph{virtio-net}). The driver source can be found in the linux kernel sources (drivers/net/virtio\_net.c), while the device
emulation can be found in the the QEMU source (hw/virtio-net.c).
Since a network adapter deals with two independent data streams, one for TX and one for RX, two virtual queues are employed. The virtio
block device uses just one virtual queue because the two streams are not independent, but are alwasy request and response.

\vspace{0.5cm}

For each virtual queue, two PCCs can be defined: the \emph{direct} PCC and the \emph{inverse} PCC, depending on who is the communication
master.
On the TX path the guest is the master: in the TX direct PCC the guest produces TX avail buffers to send and the hypervisor consumes (sends)
avail buffers, while in the TX reverse PCC the hypervisor produces used buffers and the guest consumes (frees) used buffers.
On the RX path the hypervisor is the master: in the RX direct PCC the hypervisor produces (receives into) RX used buffers and the guest
consumes (receives from) RX used buffers, while in the RX reverse PCC the guest produces RX avail buffers and the hypervisor uses RX
avail buffers. This situation is depicted in figure \ref{fig:pccs}.

\begin{figure}[bt]
\centering
\includegraphics[scale = 0.55]{pccs.pdf}
\caption{On the top there is the sender (TX) virtual queue with its two Producer/Consumer Couples (PCCs). On the bottom there is the
	 receuver (RX) virtual queue with its PCCs.}
\label{fig:pccs}
\end{figure}


\subsection{TX path}
\label{sec:virtionet-tx}
The TX path uses the sender virtual queue (\texttt{svq}), where the virtual queue callback is initially disabled on the driver side.
When the kernel invokes the \texttt{ndo\_start\_xmit} method, it does the following
\begin{enumerate}
  \item Free any pending used buffers in the used ring repeatedly calling \texttt{svq.get\_buf()}. The token returned is a pointer to a
	\texttt{sk\_buff} that was previously added to the avail ring, so that the \texttt{sk\_buff} can be freed with
	\texttt{dev\_free\_skb(skb)}.
  \item Convert the \texttt{sk\_buff} provided as argument to a SG array. The SG array will have more elements if the
	\texttt{sk\_buff} is nonlinear or just an element if linear.
  \item Invoke \texttt{svq.add\_buf(sg, skb)} to insert a new output SG in the avail ring.
  \item Invoke \texttt{svq.kick()} to notify the hypervisor.
  \item Call \texttt{skb\_orphan(skb)} so that the kernel won't wait for the \texttt{sk\_buff} to be freed and so won't invoke the
	TX watchdog.
  \item If there is no space in the virtual queue for the next send, tell the kernel to stop making new TX requests by calling
	\texttt{netif\_stop\_queue()} and invoke \texttt{svq.enable\_cb\_delayed()}\footnote{This is a variant of the \texttt{enable\_cb}
	method that delays the abilitation in the future. The variant is implented only if the Virtio EVENT\_IDX feature is set. However,
	you cannot specify how far in the future the abilitation will take place.}.
	We will be notified by the hypervisor when this pushes more used buffer to the used ring, so that we can call \texttt{get\_buf()}
	to free queue space and add more new TX requests.
	After notification are enabled we must do a double check for more used buffers otherwise we would incur the same race condition
	described in section \ref{sec:virtiomin}. If there are more used buffers, we free them (in the same way as we do at point 1) and,
	if now there is space enough for a new send, we disable the callback and tell the kernel it can restart making TX request (invoking
	the \texttt{netif\_wake\_queue()}).
\end{enumerate}
Note that while the avail ring is not full (e.g. while the \texttt{add\_buf} is successful) the sender virtual queue callback is never used,
since the used buffers are cleaned at the beginning of the the \texttt{ndo\_start\_xmit} method.
However, when there is no more space, the kernel is told not to call this method anymore, and so the only way to free used buffer after that
is to use the callback.

\vspace{0.5cm}

The device emulator is implemented with the same scheme presented in section \ref{sec:virtiomin} (see figure \ref{fig:virtiocode}), where
the guest notification are initially enabled.
In this way the guest notification are mitigated when the guest sender is faster than the QEMUBH TX processing cycle.
The TX processing is done basically calling \texttt{qemu\_sendv\_packet(sg)}\footnote{This is a simple variant of the
\texttt{qemu\_send\_packet} function, that accepts a scatter-gather array.}, where \texttt{sg} is the SG list extracted by the avail ring.
A copy is not performed in the hypervisor, but is necessary to map the guest physical memory onto the hypervisor virtual memory, since the
virtio descriptors contains physical addresses. This mapping, however, is done internally by the \texttt{virtqueue.pop()}, so it's not
visible to the device emulator.


\subsection{RX path}
The RX path uses the receiver virtual queue (\texttt{rvq}), where the virtual queue callback is initially enabled on the driver side.
At initialization time, the driver must provide the hypervisor with some receive buffers, otherwise it cannot accept incoming
packets\footnote{We've seen the same problem with e1000 in section \ref{sec:rxdriver}}. In other words, the driver has to fill the avail
ring with receive buffers calling \texttt{add\_buf()} as many times as possible. A pointer to an empty \texttt{sk\_buff} is passed as
token.

\vspace{0.5cm}

On the device emulation side (virtio-net frontend), the guest notification are initially disabled.
When the hypervisor receives a packet, the network backend invokes the \texttt{virtio\_net\_receive} method, that performs the
following actions:
\begin{enumerate}
  \item If there are no receive buffers to use, enable guest notifications, so that we will be notified when more receive buffers
	are added to the avail ring. Then return 0 (the number of bytes received). When the QEMU network core sees that the virtio-net
	frontend is not able to receive the packet, it puts the received packets into an internal queue. When the guests notification
	comes, the frontend invokes \texttt{qemu\_flush\_queued\_packets}, so that the QEMU network core tries again to call
	\texttt{virtio\_net\_receive} on each queued packet.
	After we enable notifications, we must double check for more avail buffers, otherwise the usual race condition can show up.
  \item Pop a receive buffer from the virtual queue (\texttt{rvq.pop(sg)}).
  \item Copy in this receive buffer the ethernet frame passed to \texttt{virtio\_net\_receive}.
  \item Pushes the used buffer to the used ring. To be precise, the push operation is not performed with the \texttt{push()} method, 
	but in two (or more) splitted calls. Firstly the \texttt{fill()} method is called to put an used SG list in the ring without
	updating the user ring index, and then the \texttt{flush()} method is called to update the used ring index. The reason for this
	is that in general we need more avail SG lists (and so more \texttt{pop}) for a single ethernet frame. Since we don't want to
	expose an used index that corrispond to a incomplete ethernet frame we can update the used index only when the received packets
	is completely in the used ring.
  \item Notify the guest with \texttt{rvq.notify()}.
\end{enumerate}

\vspace{0.5cm}

The virtio driver is implemented with the same scheme presented in section \ref{sec:virtiomin}. The NAPI polling function is scheduled by
the receiver virtual queue callback, which also disable further callbacks. The polling function polls for used receive buffers calling
\texttt{rvq.get\_buf()}, that returns the \texttt{sk\_buff} token. Each used buffer is processed by the \texttt{receive\_buf} function,
which invokes the \texttt{netif\_receive\_skb} as last operation.
When the working cycle exits, it means that the NAPI budget is over or that no more used buffer are ready.
At this point the polling function has made room in the avail ring and so refills it with new receive buffers\footnote{Observe that the
receive buffer management is very similar to the one employed by e1000.}.

At this point the callback is disabled and the usual double check performed (with callback enabling and NAPI rescheduling if there are
more used buffers).


\subsection{Other details}
In this section we've outlined the most important details about \emph{virtio-net} implementation. However, we have skipped some details
such as out-of-memory (OOM) managment, and receive buffer allocation/deallocation and processing.
In fact three type of receive buffers can be used: \emph{small} buffers, \emph{big} buffers and \emph{mergeable} buffers. For more
information refer to the source code.

\vspace{0.5cm}

Another important feature is the use of TSO/GSO\footnote{Generic Segmentation Offload is a network driver feature that extends the TSO
concept to other protocols, such as UDP.} kernel support (introduced in section \ref{sec:e1000-hardware}), that allows to transmit
and receive very big packets (up to about 64KB), which entails huge improvements in TCP throughput compared with the case in which
GSO packets are not used.


\subsection{Performance analysis}
In this section we will preform the same experiments presented in section \ref{sec:e1000perf}, so that we can compare the result with
the e1000 performance (patched or unpatched.

\subsubsection{TX performance}
\label{sec:virtionet-perf-tx}
The results in 1-VCPU case are shown in table \ref{tab:virtionet-tx-g2h1vcpu}. We can see that the interrupt rate is very low and the
performance is good ($\sim 160$ Kpps). At the same time, the TX notification rate is almost zero. 

\begin{table}
\begin{center}
\begin{tabular}{lrl}
\toprule
\textbf{Interrupt rate} & 0.822 & KHz\\
\textbf{TX packet rate} & 158.115 & Kpps\\
\textbf{TX bitrate} & 130.287 & Mbps\\
\textbf{TX notifications} & 0.007 & Mbps\\
\bottomrule
\end{tabular}
\end{center}
\caption{Guest to host statistics with 1 VCPU per guest.}
\label{tab:virtionet-tx-g2h1vcpu}
\end{table}

What happens here is that the producer (the guest) is faster than the consumer (the hypervisor) in the TX direct PCC: This is why the 
hypervisor (almost) never needs to exit from its working cycle and so (almost) never enables the guest notification. 
Since the guest is faster, the sender virtual queue 
should be always close to full: As soon as the hypervisor makes some room, the guest should replenish it, tell the kernel to stop 
sending packets, and reenable interrupts. In the TX reverse PCC, in other words, the consumer (the guest) is faster than the producer 
(the hypervisor).
In this scenario, therfore, the interrupt rate should be very high, because the system would
enter in a permanent \emph{almost-full} state:
\begin{enumerate}
  \item The sender virtual queue has space for only one (or a very few) packet, therefore rapidly replenishes the queue and enables the
	interrupts.
  \item As soon as the hypervisor processes the next TX packet (making room in the queue), it notifies the guest. The cycle go on from
	point 1.
\end{enumerate}
In this situation we should have nearly one interrupt per packet, and so bad performances. However, from table 
\ref{tab:virtionet-tx-g2h1vcpu} we don't see this circumstance because of the way the interrupts are enabled. As we have seen in 
section \ref{sec:virtionet-tx}, the \texttt{enable\_cb\_delayed} method is used in place of \texttt{enable\_cb}. In this way the interrupts
are enabled after a while, and not immediately. In the current implementation, the interrupt are enabled when the hypervisor has processed
$\frac{3}{4}$ of the pending packets in the queue\footnote{in our case the queue is full}. Therefore when an interrupt comes, 75\% of the
queue is empty, and not just one packet. In other words, the system enters a state in which the guest is notified every 192
packets\footnote{The current default size for a virtual queue is 256, and so $256 \cdot 0.75 = 192$.}, and still the queue is never
empty (so the hypervisor never stops) because the guest is fast to replenish the queue. In conclusion we should expect an interrupt rate of
$\frac{158.115 Kpps}{192} = 0.823$ KHz, which is exactly what we have in the table.

\vspace{0.5cm}

Although not implemented with timers, the \texttt{enable\_cb\_delayed} method is just a different \emph{incomplete} form of interrupt 
mitigation.
It is used to moderate the notification rate within a PCC where the consumer is faster than the producer: In our case
the producer is the hypervisor, which produces used buffers, and the consumer is the driver, which consumes (free) used buffers.
In order to see the mitigation effects, we have tried to run the same experiment with a modified virtio-net driver, where \texttt{enable\_cb}
is used in place of \texttt{enable\_cb\_delayed}. The results are shown in table \ref{tab:virtionet-tx-nomit-g2h1vcpu}.

\begin{table}
\begin{center}
\begin{tabular}{lrl}
\toprule
\textbf{Interrupt rate} & 108.420 & KHz\\
\textbf{TX packet rate} & 110.440 & Kpps\\
\textbf{TX bitrate} & 91.003 & Mbps\\
\textbf{TX notifications} & 0.007 & Mbps\\
\bottomrule
\end{tabular}
\end{center}
\caption{Guest to host statistics with 1 VCPU per guest, when the \texttt{enable\_cb\_delayed} method is not implemented}
\label{tab:virtionet-tx-nomit-g2h1vcpu}
\end{table}

As we can see, here the situation is exactly the one we described previously (the almost-full state), with an interrupt for each TX packet.
The TX packet rate oscillates between 80 Kpps and 150 Kpps, the system is very unstable and has a very low responsiveness: Interrupt
mitigation was definitely required in this case.

\vspace{0.5cm}

It's very important to observe that you cannot always use this form of mitigation, because of its incompleteness: there is not a 
mechanism to force timely notification of pending events. Even worse, it can be the case that a pending event stalls forever in a 
queue.
In our case, the TX reverse PCC, there is not such a problem, for two reasons:
\begin{itemize}
  \item On the TX path, the comunication \emph{master} is the guest, and so when it calls \texttt{enable\_cb\_delayed()} we are sure
	that the pending TX buffers will be processed by the \emph{slave} (the hypervisor) as soon as possible, and so the interrupt
	will be sent for sure, e.g. the pending events cannot stall.
	
  \item Considering the way the TX path is implemented (see section \ref{sec:virtionet-tx}), we don't care about having used TX buffers
	freed as soon as possible, since they are basically freed when needed.
\end{itemize}
In the RX path we are not so lucky (see section \ref{sec:virtionet-perf-rx}).

\vspace{0.5cm}

The results for the 2-VCPU test case are shown in table \ref{tab:virtionet-tx-g2h2vcpu}, and are very similar to results for 1-VCPU case,
since there is no significant work that can be done in parallel.

\begin{table}
\begin{center}
\begin{tabular}{lrl}
\toprule
\textbf{Interrupt rate} & 0.797 & KHz\\
\textbf{TX packet rate} & 154.816 & Kpps\\
\textbf{TX bitrate} & 127.569 & Mbps\\
\textbf{TX notifications} & 0.007 & Mbps\\
\bottomrule
\end{tabular}
\end{center}
\caption{Guest to host statistics with 2 VCPU per guest}
\label{tab:virtionet-tx-g2h2vcpu}
\end{table}


\subsubsection{RX performance}
\label{sec:virtionet-perf-rx}
The measured critical rate is about 110 Kpps, which is not very high if compared with what we get with e1000 (adding the mitigation patch).

\begin{table}
\begin{center}
\begin{tabular}{lrl}
\toprule
\textbf{Interrupt rate} & 57.615 & KHz\\
\textbf{RX packet rate} & 103.042 & Kpps\\
\textbf{RX stream} & 127.569 & Mbps\\
\textbf{RX notification} & 0.007 & Mbps\\
\bottomrule
\end{tabular}
\end{center}
\caption{Host to guest statistics with 1 VCPU per guest}
\label{tab:virtionet-rx-g1hvcpu}
\end{table}

Also in this case, the reason for low performance is tied to the lack of interrupt mitigation. As we can see, the interrupt rate is
very high (57 KHz), because Virtio does not limit the interrupt rate in any way. From an high interrupt rate we can infer that in the 
RX direct PCC, the consumer (the NAPI) is faster than the producer (hypervisor): This results in only about 2
packets processed by the polling function for each interrupt.
We could add mitigation in the same way done for the TX inverse PCC (section \ref{sec:virtionet-perf-tx}): Using
the \texttt{enable\_cb\_delayed} method in place of the \texttt{enable\_cb} method when exiting from the NAPI working cycle.
However, this simply cannot be done:
\begin{itemize}
  \item On the RX path, the \emph{master} is the hypervisor and so the guest (the\emph{slave}), when calling
	\texttt{enable\_cb\_delayed()}, cannot know when the next packet will be received by the hypervisor, and therefore cannot know
	when a total of 192 packets will be received. Since the Virtio mitigation is incomplete, up to 191 RX packets could stall
	in the used ring. Of course, this is not acceptable.
  \item Latency is affected negatively by the mitigation in this case.
\end{itemize}
A complete form of mitigation is here the only solution.

\vspace{0.5cm}

On the RX reverse PCC the consumer (hypervisor) is slower than the producer (hypervisor) and consequently the RX notification rate is
very good (about zero notifications).

\vspace{0.5cm}

The measured results are shown in table \ref{tab:virtionet-rx-g2hvcpu}. There is not a substantial difference with the 1-VCPU case, because
there is not enough parallelism in the guest to exploit.

\begin{table}
\begin{center}
\begin{tabular}{lrl}
\toprule
\textbf{Interrupt rate} & 43.542 & KHz\\
\textbf{RX packet rate} & 100.142 & Kpps\\
\textbf{TX stream} & 82.517 & Mbps\\
\textbf{RX notification} & 0.004 & Mbps\\
\bottomrule
\end{tabular}
\end{center}
\caption{Host to guest statistics with 2 VCPU per guest}
\label{tab:virtionet-rx-g2hvcpu}
\end{table}
